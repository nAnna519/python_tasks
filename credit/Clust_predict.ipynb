{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(data, unique_home_ownership=None, unique_verification=None, dict_grade=None,dict_sub_grade=None, dict_emp_title=None, dict_pymnt_plan=None, dict_purpose=None):\n",
    "    numeric_data = data.drop(['record_id', 'earliest_cr_line', 'issue_d', 'addr_state', 'zip_code'], axis=1)\n",
    "    \n",
    "    numeric_data['term'] = numeric_data['term'].map({' 36 months': 0, ' 60 months': 1})\n",
    "    numeric_data['application_type'] = numeric_data['application_type'].map({'INDIVIDUAL': 0, 'JOINT': 1})\n",
    "    numeric_data['initial_list_status'] = numeric_data['initial_list_status'].map({'f': 0, 'w': 1})\n",
    "    numeric_data['emp_length'] = numeric_data['emp_length'].map({'< 1 year': 1, '1 year': 2, '2 years': 3,  '3 years': 4,  '4 years': 5,  '5 years': 6,  '6 years': 7,  '7 years': 8,  '8 years': 9,  '9 years': 10,  '10+ years': 11})\n",
    "\n",
    "    numeric_data['emp_length'].fillna(0, inplace=True)\n",
    "    numeric_data['emp_title'].fillna('0', inplace=True)\n",
    "    numeric_data['mths_since_last_delinq'].fillna(numeric_data['mths_since_last_delinq'].notnull().min())\n",
    "    numeric_data['collections_12_mths_ex_med'].fillna(numeric_data['collections_12_mths_ex_med'].notnull().max())\n",
    "    numeric_data['revol_util'].fillna(numeric_data['revol_util'].notnull().mean())\n",
    "    numeric_data['tot_coll_amt'].fillna(numeric_data['tot_coll_amt'].notnull().min())\n",
    "    numeric_data['tot_cur_bal'].fillna(numeric_data['tot_cur_bal'].notnull().min())\n",
    "    numeric_data['total_rev_hi_lim'].fillna(numeric_data['total_rev_hi_lim'].notnull().min())\n",
    "  \n",
    "    home_ownership_list = numeric_data.home_ownership.values.reshape(-1, 1)\n",
    "\n",
    "    numeric_data = pd.concat((numeric_data,pd.get_dummies(numeric_data.home_ownership)),1)\n",
    "    \n",
    "    if unique_home_ownership is None:\n",
    "        unique_home_ownership = numeric_data['home_ownership'].unique()\n",
    "        \n",
    "    for i in unique_home_ownership:\n",
    "        try:\n",
    "            numeric_data[i]\n",
    "        except KeyError:\n",
    "            numeric_data[i] = 0\n",
    "    \n",
    "    verification_list = numeric_data.verification_status.values.reshape(-1, 1)\n",
    "\n",
    "    numeric_data = pd.concat((numeric_data,pd.get_dummies(numeric_data.verification_status)),1)\n",
    "    \n",
    "    if unique_verification is None:\n",
    "        unique_verification = numeric_data['verification_status'].unique()\n",
    "        \n",
    "    for i in unique_verification:\n",
    "        try:\n",
    "            numeric_data[i]\n",
    "        except KeyError:\n",
    "            numeric_data[i] = 0\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "\n",
    "    if dict_grade is None:\n",
    "        le.fit(numeric_data.grade.astype(str))\n",
    "        numeric_data['grade_le'] = le.transform((numeric_data['grade'].values))\n",
    "        dict_grade = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    else:\n",
    "        numeric_data['grade_le'] = numeric_data['grade'].map(dict_grade).fillna(-1)  \n",
    "\n",
    "    if dict_sub_grade is None:\n",
    "        le.fit(numeric_data.sub_grade.astype(str))\n",
    "        numeric_data['sub_grade_le'] = le.transform(numeric_data['sub_grade'].values)\n",
    "        dict_sub_grade = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    else:\n",
    "        numeric_data['sub_grade_le'] = numeric_data['sub_grade'].map(dict_sub_grade).fillna(-1)  \n",
    "    \n",
    "    if dict_emp_title is None:\n",
    "        le.fit(numeric_data.emp_title.astype(str))\n",
    "        numeric_data['emp_title_le'] = le.transform(numeric_data['emp_title'].values)\n",
    "        dict_emp_title = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    else:\n",
    "        numeric_data['emp_title_le'] = numeric_data['emp_title'].map(dict_emp_title).fillna(-1)        \n",
    "\n",
    "    if dict_pymnt_plan is None:\n",
    "        le.fit(numeric_data.pymnt_plan.astype(str))\n",
    "        numeric_data['pymnt_plan_le'] = le.transform(numeric_data['pymnt_plan'].values)\n",
    "        dict_pymnt_plan = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    else:\n",
    "        numeric_data['pymnt_plan_le'] = numeric_data['pymnt_plan'].map(dict_pymnt_plan).fillna(0)  \n",
    "    \n",
    "    if dict_purpose is None:\n",
    "        le.fit(numeric_data.purpose.astype(str))\n",
    "        numeric_data['purpose_le'] = le.transform(numeric_data['purpose'].values)\n",
    "        dict_purpose = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "#         dict_purpose[None] = 5\n",
    "    else:\n",
    "        numeric_data['purpose_le'] = numeric_data['purpose'].map(dict_purpose).fillna(-1) \n",
    "#     print(dict_grade)\n",
    "\n",
    "    numeric_data = numeric_data.drop(['grade', 'sub_grade', 'purpose', 'home_ownership','emp_title', 'pymnt_plan',  'verification_status'], axis=1)\n",
    "    \n",
    "    return numeric_data, unique_home_ownership, unique_verification, dict_grade, dict_sub_grade, dict_emp_title, dict_pymnt_plan, dict_purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, unique_home_ownership, unique_verification, dict_grade, dict_sub_grade, dict_emp_title, dict_pymnt_plan, dict_purpose = preprocessing_data(train)\n",
    "test = preprocessing_data(test,unique_home_ownership, unique_verification, dict_grade, dict_sub_grade, dict_emp_title, dict_pymnt_plan, dict_purpose)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = new_data\n",
    "X_train = train.drop(['loan_status'], axis=1)\n",
    "y_train = train[['loan_status']]\n",
    "\n",
    "X_test = test.drop(['loan_status'], axis=1)\n",
    "y_test = test[['loan_status']]\n",
    "\n",
    "# X_train = scale(X_train)\n",
    "# X_test = scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NikitsinskayaH\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=32)\n",
    "preds_train = kmeans.fit_predict(X_train[['dti', 'int_rate', 'emp_length', 'loan_amnt', 'installment']])\n",
    "\n",
    "X_train['clusters'] = preds_train\n",
    "y_train['clusters'] = preds_train\n",
    "# preds_train = np.array(preds_train).reshape(int(X_train.size / 7), 1)\n",
    "\n",
    "# X_train = np.hstack((X_train, preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0 = X_train[X_train['clusters'] == 0]\n",
    "train_1 = X_train[X_train['clusters'] == 1]\n",
    "train_2 = X_train[X_train['clusters'] == 2]\n",
    "\n",
    "train_0_y = y_train[y_train['clusters'] == 0]\n",
    "train_1_y = y_train[y_train['clusters'] == 1]\n",
    "train_2_y = y_train[y_train['clusters'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51986\n",
      "66531\n",
      "21615\n"
     ]
    }
   ],
   "source": [
    "print(len(train_0))\n",
    "print(len(train_1))\n",
    "print(len(train_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_train_0, TEST_train_0, TRAIN_train_0_y, TEST_train_0_y = train_test_split(train_0, train_0_y, test_size=0.3, random_state=42)\n",
    "TRAIN_train_1, TEST_train_1, TRAIN_train_1_y, TEST_train_1_y = train_test_split(train_1, train_1_y, test_size=0.3, random_state=42)\n",
    "TRAIN_train_2, TEST_train_2, TRAIN_train_2_y, TEST_train_2_y = train_test_split(train_2, train_2_y, test_size=0.3, random_state=42)\n",
    "# TRAIN_train_3, TEST_train_3, TRAIN_train_3_y, TEST_train_3_y = train_test_split(train_3, train_3_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: \n",
      "Accuracy score: 0.7710951526032316\n",
      "Recall score: 0.9775328172332548\n",
      "Precision score: 0.7786193029490617\n",
      "Auc score: 0.7272584562045172\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "cls_0 = xgb.XGBClassifier()\n",
    "cls_0.fit(TRAIN_train_0, TRAIN_train_0_y['loan_status'])\n",
    "preds = cls_0.predict(TEST_train_0)\n",
    "print('XGB: ')\n",
    "print('Accuracy score: ' + str(metrics.accuracy_score(TEST_train_0_y['loan_status'], preds)))\n",
    "\n",
    "print('Recall score: ' + str(metrics.recall_score(TEST_train_0_y['loan_status'], preds)))\n",
    "\n",
    "print('Precision score: ' + str(metrics.precision_score(TEST_train_0_y['loan_status'], preds)))\n",
    "\n",
    "preds = cls_0.predict_proba(TEST_train_0)\n",
    "preds = preds[:,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(TEST_train_0_y['loan_status'], preds)\n",
    "print('Auc score: ' + str(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: \n",
      "Accuracy score: 0.8061623246492986\n",
      "Recall score: 0.9910224438902743\n",
      "Precision score: 0.8101523877478212\n",
      "Auc score: 0.700189274899486\n"
     ]
    }
   ],
   "source": [
    "cls_1 = xgb.XGBClassifier()\n",
    "cls_1.fit(TRAIN_train_1, TRAIN_train_1_y['loan_status'])\n",
    "preds = cls_1.predict(TEST_train_1)\n",
    "print('XGB: ')\n",
    "print('Accuracy score: ' + str(metrics.accuracy_score(TEST_train_1_y['loan_status'], preds)))\n",
    "\n",
    "print('Recall score: ' + str(metrics.recall_score(TEST_train_1_y['loan_status'], preds)))\n",
    "\n",
    "print('Precision score: ' + str(metrics.precision_score(TEST_train_1_y['loan_status'], preds)))\n",
    "\n",
    "preds = cls_0.predict_proba(TEST_train_1)\n",
    "preds = preds[:,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(TEST_train_1_y['loan_status'], preds)\n",
    "print('Auc score: ' + str(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: \n",
      "Accuracy score: 0.7312259059367772\n",
      "Recall score: 0.9442671771963412\n",
      "Precision score: 0.7498310810810811\n",
      "Auc score: 0.6979118077157518\n"
     ]
    }
   ],
   "source": [
    "cls_2 = xgb.XGBClassifier()\n",
    "cls_2.fit(TRAIN_train_2, TRAIN_train_2_y['loan_status'])\n",
    "preds = cls_2.predict(TEST_train_2)\n",
    "print('XGB: ')\n",
    "print('Accuracy score: ' + str(metrics.accuracy_score(TEST_train_2_y['loan_status'], preds)))\n",
    "\n",
    "print('Recall score: ' + str(metrics.recall_score(TEST_train_2_y['loan_status'], preds)))\n",
    "\n",
    "print('Precision score: ' + str(metrics.precision_score(TEST_train_2_y['loan_status'], preds)))\n",
    "\n",
    "preds = cls_0.predict_proba(TEST_train_2)\n",
    "preds = preds[:,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(TEST_train_2_y['loan_status'], preds)\n",
    "print('Auc score: ' + str(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_3 = xgb.XGBClassifier()\n",
    "# cls_3.fit(TRAIN_train_3, TRAIN_train_3_y)\n",
    "# preds = cls_3.predict(TEST_train_3)\n",
    "# print('XGB: ')\n",
    "# print('Accuracy score: ' + str(metrics.accuracy_score(TEST_train_3_y, preds)))\n",
    "\n",
    "# print('Recall score: ' + str(metrics.recall_score(TEST_train_3_y, preds)))\n",
    "\n",
    "# print('Precision score: ' + str(metrics.precision_score(TEST_train_3_y, preds)))\n",
    "\n",
    "# print('Auc score: ' + str(metrics.roc_auc_score(TEST_train_3_y, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NikitsinskayaH\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "preds_test = kmeans.predict(X_test[['dti', 'int_rate', 'emp_length', 'loan_amnt', 'installment']])\n",
    "\n",
    "X_test['clusters'] = preds_test\n",
    "y_test['clusters'] = preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_0 = X_test[X_test['clusters'] == 0]\n",
    "test_1 = X_test[X_test['clusters'] == 1]\n",
    "test_2 = X_test[X_test['clusters'] == 2]\n",
    "\n",
    "test_0_y = y_test[y_test['clusters'] == 0]\n",
    "test_1_y = y_test[y_test['clusters'] == 1]\n",
    "test_2_y = y_test[y_test['clusters'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: \n",
      "Accuracy score: 0.7722638882651457\n",
      "Recall score: 0.9586598058706584\n",
      "Precision score: 0.7897779276458403\n",
      "Auc score: 0.7150373596924235\n"
     ]
    }
   ],
   "source": [
    "test_0 = test_0[TRAIN_train_0.columns]\n",
    "preds_1 = cls_0.predict(test_0)\n",
    "\n",
    "preds_1 = list(preds_1)\n",
    "print('XGB: ')\n",
    "print('Accuracy score: ' + str(metrics.accuracy_score(test_0_y['loan_status'], preds_1)))\n",
    "\n",
    "print('Recall score: ' + str(metrics.recall_score(test_0_y['loan_status'], preds_1)))\n",
    "\n",
    "print('Precision score: ' + str(metrics.precision_score(test_0_y['loan_status'], preds_1)))\n",
    "\n",
    "preds_1 = cls_0.predict_proba(test_0)\n",
    "preds_1 = preds_1[:,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(test_0_y['loan_status'], preds_1)\n",
    "print('Auc score: ' + str(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: \n",
      "Accuracy score: 0.8025369244135534\n",
      "Recall score: 0.9842482100238663\n",
      "Precision score: 0.8100424984821971\n",
      "Auc score: 0.7004053719920469\n"
     ]
    }
   ],
   "source": [
    "test_1 = test_1[TRAIN_train_1.columns]\n",
    "preds_2 = cls_1.predict(test_1)\n",
    "preds_2 = list(preds_2)\n",
    "print('XGB: ')\n",
    "print('Accuracy score: ' + str(metrics.accuracy_score(test_1_y['loan_status'], preds_2)))\n",
    "\n",
    "print('Recall score: ' + str(metrics.recall_score(test_1_y['loan_status'], preds_2)))\n",
    "\n",
    "print('Precision score: ' + str(metrics.precision_score(test_1_y['loan_status'], preds_2)))\n",
    "\n",
    "preds_2 = cls_1.predict_proba(test_1)\n",
    "preds_2 = preds_2[:,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(test_1_y['loan_status'], preds_2)\n",
    "print('Auc score: ' + str(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: \n",
      "Accuracy score: 0.7393233499722685\n",
      "Recall score: 0.9489380930863082\n",
      "Precision score: 0.7580315244856215\n",
      "Auc score: 0.6941674109169215\n"
     ]
    }
   ],
   "source": [
    "test_2 = test_2[TRAIN_train_2.columns]\n",
    "preds_3 = cls_2.predict(test_2)\n",
    "preds_3 = list(preds_3)\n",
    "print('XGB: ')\n",
    "print('Accuracy score: ' + str(metrics.accuracy_score(test_2_y['loan_status'], preds_3)))\n",
    "\n",
    "print('Recall score: ' + str(metrics.recall_score(test_2_y['loan_status'], preds_3)))\n",
    "\n",
    "print('Precision score: ' + str(metrics.precision_score(test_2_y['loan_status'], preds_3)))\n",
    "\n",
    "preds_3 = cls_2.predict_proba(test_2)\n",
    "preds_3 = preds_3[:,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(test_2_y['loan_status'], preds_3)\n",
    "print('Auc score: ' + str(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_4 = cls_3.predict(test_3)\n",
    "# preds_4 = list(preds_4)\n",
    "# print('XGB: ')\n",
    "# print('Accuracy score: ' + str(metrics.accuracy_score(test_3_y, preds_4)))\n",
    "\n",
    "# print('Recall score: ' + str(metrics.recall_score(test_3_y, preds_4)))\n",
    "\n",
    "# print('Precision score: ' + str(metrics.precision_score(test_3_y, preds_4)))\n",
    "\n",
    "# print('Auc score: ' + str(metrics.roc_auc_score(test_3_y, preds_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc score: 0.7079491642773359\n"
     ]
    }
   ],
   "source": [
    "preds = np.append(preds_1, preds_2)\n",
    "preds = np.append(preds, preds_3)\n",
    "\n",
    "test_y = np.append(test_0_y['loan_status'], test_1_y['loan_status'])\n",
    "test_y = np.append(test_y, test_2_y['loan_status'])\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(test_y, preds)\n",
    "print('Auc score: ' + str(metrics.auc(fpr, tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
