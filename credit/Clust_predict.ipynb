{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame()\n",
    "\n",
    "new_data['initial_list_status'] = data['initial_list_status'].map({'f': 0, 'w': 1})\n",
    "new_data['dti'] = data['dti']\n",
    "new_data['int_rate'] = data['int_rate']\n",
    "new_data['emp_length'] = data['emp_length'].map({'< 1 year': 1, '1 year': 2, '2 years': 3,  '3 years': 4,  '4 years': 5,  '5 years': 6,  '6 years': 7,  '7 years': 8,  '8 years': 9,  '9 years': 10,  '10+ years': 11})\n",
    "new_data['emp_length'].fillna(0, inplace=True)\n",
    "\n",
    "new_data['tot_cur_bal'] = data['tot_cur_bal']\n",
    "new_data['tot_cur_bal'].fillna(new_data['tot_cur_bal'].notnull().min(), inplace=True)\n",
    "new_data['loan_amnt'] = data['loan_amnt']\n",
    "new_data['loan_status'] = data['loan_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by the scale function.\n",
      "  \"\"\"\n",
      "/home/anna/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by the scale function.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# X = new_data\n",
    "X = new_data.drop(['loan_status'], axis=1)\n",
    "y = new_data['loan_status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train = scale(X_train)\n",
    "X_test = scale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=32)\n",
    "preds_train = kmeans.fit_predict(X_train)\n",
    "\n",
    "# preds_train = np.array(preds_train).reshape(int(X_train.size / 7), 1)\n",
    "\n",
    "# X_train = np.hstack((X_train, preds_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_0 = np.array([])\n",
    "train_1 = np.array([])\n",
    "train_2 = np.array([])\n",
    "# train_3 = np.array([])\n",
    "\n",
    "train_0_y = []\n",
    "train_1_y = []\n",
    "train_2_y = []\n",
    "# train_3_y = []\n",
    "\n",
    "y_train = list(y_train)\n",
    "i = 0\n",
    "# print(list(y_train)[0])\n",
    "for cluster in preds_train:\n",
    "#     print(cluster)\n",
    "    \n",
    "    if cluster == 0:\n",
    "        train_0 = np.append(train_0, list(X_train[i]))\n",
    "        train_0_y.append(y_train[i])\n",
    "    elif cluster == 1:\n",
    "        train_1 = np.append(train_1, list(X_train[i]))\n",
    "        train_1_y.append(y_train[i])\n",
    "    elif cluster == 2:\n",
    "        train_2 = np.append(train_2, list(X_train[i]))\n",
    "        train_2_y.append(y_train[i])\n",
    "#     elif cluster == 3:\n",
    "#         train_3 = np.append(train_3, list(X_train[i]))      \n",
    "#         train_3_y.append(y_train[i])\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "train_0 = train_0.reshape(int(train_0.size / 6), 6)\n",
    "train_1 = train_1.reshape(int(train_1.size / 6), 6)\n",
    "train_2 = train_2.reshape(int(train_2.size / 6), 6)\n",
    "# train_3 = train_3.reshape(int(train_3.size / 6), 6)\n",
    "\n",
    "# print(train_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_train_0, TEST_train_0, TRAIN_train_0_y, TEST_train_0_y = train_test_split(train_0, train_0_y, test_size=0.3, random_state=42)\n",
    "TRAIN_train_1, TEST_train_1, TRAIN_train_1_y, TEST_train_1_y = train_test_split(train_1, train_1_y, test_size=0.3, random_state=42)\n",
    "TRAIN_train_2, TEST_train_2, TRAIN_train_2_y, TEST_train_2_y = train_test_split(train_2, train_2_y, test_size=0.3, random_state=42)\n",
    "# TRAIN_train_3, TEST_train_3, TRAIN_train_3_y, TEST_train_3_y = train_test_split(train_3, train_3_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: \n",
      "Accuracy score: 0.8191134192710748\n",
      "Recall score: 0.9959814353633688\n",
      "Precision score: 0.8212535585942969\n",
      "Auc score: 0.5067131394364304\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "cls_0 = xgb.XGBClassifier()\n",
    "cls_0.fit(TRAIN_train_0, TRAIN_train_0_y)\n",
    "preds = cls_0.predict(TEST_train_0)\n",
    "print('XGB: ')\n",
    "print('Accuracy score: ' + str(metrics.accuracy_score(TEST_train_0_y, preds)))\n",
    "\n",
    "print('Recall score: ' + str(metrics.recall_score(TEST_train_0_y, preds)))\n",
    "\n",
    "print('Precision score: ' + str(metrics.precision_score(TEST_train_0_y, preds)))\n",
    "\n",
    "print('Auc score: ' + str(metrics.roc_auc_score(TEST_train_0_y, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: \n",
      "Accuracy score: 0.7143912997571534\n",
      "Recall score: 0.9556952552013172\n",
      "Precision score: 0.726063224926086\n",
      "Auc score: 0.5461271974931317\n"
     ]
    }
   ],
   "source": [
    "cls_1 = xgb.XGBClassifier()\n",
    "cls_1.fit(TRAIN_train_1, TRAIN_train_1_y)\n",
    "preds = cls_1.predict(TEST_train_1)\n",
    "print('XGB: ')\n",
    "print('Accuracy score: ' + str(metrics.accuracy_score(TEST_train_1_y, preds)))\n",
    "\n",
    "print('Recall score: ' + str(metrics.recall_score(TEST_train_1_y, preds)))\n",
    "\n",
    "print('Precision score: ' + str(metrics.precision_score(TEST_train_1_y, preds)))\n",
    "\n",
    "print('Auc score: ' + str(metrics.roc_auc_score(TEST_train_1_y, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: \n",
      "Accuracy score: 0.7674482006543075\n",
      "Recall score: 0.9797369356558834\n",
      "Precision score: 0.7759009009009009\n",
      "Auc score: 0.52437139180455\n"
     ]
    }
   ],
   "source": [
    "cls_2 = xgb.XGBClassifier()\n",
    "cls_2.fit(TRAIN_train_2, TRAIN_train_2_y)\n",
    "preds = cls_2.predict(TEST_train_2)\n",
    "print('XGB: ')\n",
    "print('Accuracy score: ' + str(metrics.accuracy_score(TEST_train_2_y, preds)))\n",
    "\n",
    "print('Recall score: ' + str(metrics.recall_score(TEST_train_2_y, preds)))\n",
    "\n",
    "print('Precision score: ' + str(metrics.precision_score(TEST_train_2_y, preds)))\n",
    "\n",
    "print('Auc score: ' + str(metrics.roc_auc_score(TEST_train_2_y, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cls_3 = xgb.XGBClassifier()\n",
    "# cls_3.fit(TRAIN_train_3, TRAIN_train_3_y)\n",
    "# preds = cls_3.predict(TEST_train_3)\n",
    "# print('XGB: ')\n",
    "# print('Accuracy score: ' + str(metrics.accuracy_score(TEST_train_3_y, preds)))\n",
    "\n",
    "# print('Recall score: ' + str(metrics.recall_score(TEST_train_3_y, preds)))\n",
    "\n",
    "# print('Precision score: ' + str(metrics.precision_score(TEST_train_3_y, preds)))\n",
    "\n",
    "# print('Auc score: ' + str(metrics.roc_auc_score(TEST_train_3_y, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = kmeans.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_0 = np.array([])\n",
    "test_1 = np.array([])\n",
    "test_2 = np.array([])\n",
    "# test_3 = np.array([])\n",
    "\n",
    "test_0_y = []\n",
    "test_1_y = []\n",
    "test_2_y = []\n",
    "# test_3_y = []\n",
    "\n",
    "y_test = list(y_test)\n",
    "i = 0\n",
    "# print(list(y_train)[0])\n",
    "for cluster in preds_test:\n",
    "#     print(cluster)\n",
    "    \n",
    "    if cluster == 0:\n",
    "        test_0 = np.append(test_0, list(X_test[i]))\n",
    "        test_0_y.append(y_test[i])\n",
    "    elif cluster == 1:\n",
    "        test_1 = np.append(test_1, list(X_test[i]))\n",
    "        test_1_y.append(y_test[i])\n",
    "    elif cluster == 2:\n",
    "        test_2 = np.append(test_2, list(X_test[i]))\n",
    "        test_2_y.append(y_test[i])\n",
    "#     elif cluster == 3:\n",
    "#         test_3 = np.append(test_3, list(X_test[i]))      \n",
    "#         test_3_y.append(y_test[i])\n",
    "\n",
    "    i += 1\n",
    "    \n",
    "test_0 = test_0.reshape(int(test_0.size / 6), 6)\n",
    "test_1 = test_1.reshape(int(test_1.size / 6), 6)\n",
    "test_2 = test_2.reshape(int(test_2.size / 6), 6)\n",
    "# test_3 = test_3.reshape(int(test_3.size / 6), 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: \n",
      "Accuracy score: 0.8178406186300838\n",
      "Recall score: 0.9952921628357796\n",
      "Precision score: 0.8203071505428935\n",
      "Auc score: 0.5082144650768596\n"
     ]
    }
   ],
   "source": [
    "preds_1 = cls_0.predict(test_0)\n",
    "preds_1 = list(preds_1)\n",
    "print('XGB: ')\n",
    "print('Accuracy score: ' + str(metrics.accuracy_score(test_0_y, preds_1)))\n",
    "\n",
    "print('Recall score: ' + str(metrics.recall_score(test_0_y, preds_1)))\n",
    "\n",
    "print('Precision score: ' + str(metrics.precision_score(test_0_y, preds_1)))\n",
    "\n",
    "print('Auc score: ' + str(metrics.roc_auc_score(test_0_y, preds_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: \n",
      "Accuracy score: 0.7167487684729064\n",
      "Recall score: 0.9556702116457829\n",
      "Precision score: 0.7289959839357429\n",
      "Auc score: 0.5453818886990771\n"
     ]
    }
   ],
   "source": [
    "preds_2 = cls_1.predict(test_1)\n",
    "preds_2 = list(preds_2)\n",
    "print('XGB: ')\n",
    "print('Accuracy score: ' + str(metrics.accuracy_score(test_1_y, preds_2)))\n",
    "\n",
    "print('Recall score: ' + str(metrics.recall_score(test_1_y, preds_2)))\n",
    "\n",
    "print('Precision score: ' + str(metrics.precision_score(test_1_y, preds_2)))\n",
    "\n",
    "print('Auc score: ' + str(metrics.roc_auc_score(test_1_y, preds_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB: \n",
      "Accuracy score: 0.7665693245302184\n",
      "Recall score: 0.9818514818514819\n",
      "Precision score: 0.7732249393561922\n",
      "Auc score: 0.5284925858455269\n"
     ]
    }
   ],
   "source": [
    "preds_3 = cls_2.predict(test_2)\n",
    "preds_3 = list(preds_3)\n",
    "print('XGB: ')\n",
    "print('Accuracy score: ' + str(metrics.accuracy_score(test_2_y, preds_3)))\n",
    "\n",
    "print('Recall score: ' + str(metrics.recall_score(test_2_y, preds_3)))\n",
    "\n",
    "print('Precision score: ' + str(metrics.precision_score(test_2_y, preds_3)))\n",
    "\n",
    "print('Auc score: ' + str(metrics.roc_auc_score(test_2_y, preds_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_4 = cls_3.predict(test_3)\n",
    "# preds_4 = list(preds_4)\n",
    "# print('XGB: ')\n",
    "# print('Accuracy score: ' + str(metrics.accuracy_score(test_3_y, preds_4)))\n",
    "\n",
    "# print('Recall score: ' + str(metrics.recall_score(test_3_y, preds_4)))\n",
    "\n",
    "# print('Precision score: ' + str(metrics.precision_score(test_3_y, preds_4)))\n",
    "\n",
    "# print('Auc score: ' + str(metrics.roc_auc_score(test_3_y, preds_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auc score: 0.526825064123437\n"
     ]
    }
   ],
   "source": [
    "preds = np.append(preds_1, preds_2)\n",
    "preds = np.append(preds, preds_3)\n",
    "\n",
    "test_y = np.append(test_0_y, test_1_y)\n",
    "test_y = np.append(test_y, test_2_y)\n",
    "\n",
    "\n",
    "print('Auc score: ' + str(metrics.roc_auc_score(test_y, preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
